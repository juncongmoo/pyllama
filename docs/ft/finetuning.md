

After Lora Finetuning:

```
python -m llama.llama_quant decapoda-research/llama-7b-hf c4 --load lora --bits 2 --benchmark 1024 --max_length 64 --check
ğŸŒ³ LLaMAForCausalLM
â”œâ”€â”€ model(LLaMAModel)
â”‚   â”œâ”€â”€ embed_tokens(Embedding) weight[32000,4096](fp16)(cuda:0)â„ï¸
â”‚   â”œâ”€â”€ layers(ModuleList)
â”‚   â”‚   â”œâ”€â”€ 0(LLaMADecoderLayer)
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn(LLaMAAttention)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ +q_proj,v_proj,o_proj(QuantLinear) qweight[256,4096](i32)(cuda:0)â„ï¸  zeros[4096,1](cuda:0)â„ï¸  scales[4096,1](cuda:0)â„ï¸  bias[4096](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj(MergedQuantLinear) qweight[256,4096](i32)(cuda:0)â„ï¸  zeros[4096,1](cuda:0)â„ï¸  scales[4096,1](cuda:0)â„ï¸  bias[4096](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lora_A(Linear) weight[64,4096](fp16)(cuda:0)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ lora_B(Linear) weight[4096,64](fp16)(cuda:0)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ rotary_emb(RotaryEmbedding) inv_freq[64](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp(LLaMAMLP)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ gate_proj(QuantLinear) qweight[256,11008](i32)(cuda:0)â„ï¸  zeros[11008,1](cuda:0)â„ï¸  scales[11008,1](cuda:0)â„ï¸  bias[11008](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ down_proj(MergedQuantLinear) qweight[688,4096](i32)(cuda:0)â„ï¸  zeros[4096,1](cuda:0)â„ï¸  scales[4096,1](cuda:0)â„ï¸  bias[4096](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lora_A(Linear) weight[64,11008](fp16)(cuda:0)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ lora_B(Linear) weight[4096,64](fp16)(cuda:0)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ up_proj(MergedQuantLinear) qweight[256,11008](i32)(cuda:0)â„ï¸  zeros[11008,1](cuda:0)â„ï¸  scales[11008,1](cuda:0)â„ï¸  bias[11008](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ lora_A(Linear) weight[64,4096](fp16)(cuda:0)
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ lora_B(Linear) weight[11008,64](fp16)(cuda:0)
â”‚   â”‚   â”‚   â””â”€â”€ +input_layernorm,post_attention_layernorm(RMSNorm) weight[4096](fp16)(cuda:0)â„ï¸
â”‚   â”‚   â”œâ”€â”€ 1(LLaMADecoderLayer)
â”‚   â”‚   â”‚   â”œâ”€â”€ self_attn(LLaMAAttention)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ +q_proj,v_proj,o_proj(QuantLinear) qweight[256,4096](i32)(cuda:0)â„ï¸  zeros[4096,1](cuda:0)â„ï¸  scales[4096,1](cuda:0)â„ï¸  bias[4096](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ k_proj(MergedQuantLinear) qweight[256,4096](i32)(cuda:0)â„ï¸  zeros[4096,1](cuda:0)â„ï¸  scales[4096,1](cuda:0)â„ï¸  bias[4096](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ lora_A(Linear) weight[64,4096](fp16)(cuda:0)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ lora_B(Linear) weight[4096,64](fp16)(cuda:0)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ rotary_emb(RotaryEmbedding) inv_freq[64](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”œâ”€â”€ mlp(LLaMAMLP)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ +gate_proj,up_proj(QuantLinear) qweight[256,11008](i32)(cuda:0)â„ï¸  zeros[11008,1](cuda:0)â„ï¸  scales[11008,1](cuda:0)â„ï¸  bias[11008](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ down_proj(MergedQuantLinear) qweight[688,4096](i32)(cuda:0)â„ï¸  zeros[4096,1](cuda:0)â„ï¸  scales[4096,1](cuda:0)â„ï¸  bias[4096](cuda:0)â„ï¸
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ lora_A(Linear) weight[64,11008](fp16)(cuda:0)
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ lora_B(Linear) weight[4096,64](fp16)(cuda:0)
â”‚   â”‚   â”‚   â””â”€â”€ +input_layernorm,post_attention_layernorm(RMSNorm) weight[4096](fp16)(cuda:0)â„ï¸
â”‚   â”‚   â””â”€â”€ +2-31(LLaMADecoderLayer)
â”‚   â”‚       â”œâ”€â”€ self_attn(LLaMAAttention)
â”‚   â”‚       â”‚   â”œâ”€â”€ +q_proj,k_proj,v_proj,o_proj(QuantLinear) qweight[256,4096](i32)(cuda:0)â„ï¸  zeros[4096,1](cuda:0)â„ï¸  scales[4096,1](cuda:0)â„ï¸  bias[4096](cuda:0)â„ï¸
â”‚   â”‚       â”‚   â””â”€â”€ rotary_emb(RotaryEmbedding) inv_freq[64](cuda:0)â„ï¸
â”‚   â”‚       â”œâ”€â”€ mlp(LLaMAMLP)
â”‚   â”‚       â”‚   â”œâ”€â”€ +gate_proj,up_proj(QuantLinear) qweight[256,11008](i32)(cuda:0)â„ï¸  zeros[11008,1](cuda:0)â„ï¸  scales[11008,1](cuda:0)â„ï¸  bias[11008](cuda:0)â„ï¸
â”‚   â”‚       â”‚   â””â”€â”€ down_proj(QuantLinear) qweight[688,4096](i32)(cuda:0)â„ï¸  zeros[4096,1](cuda:0)â„ï¸  scales[4096,1](cuda:0)â„ï¸  bias[4096](cuda:0)â„ï¸
â”‚   â”‚       â””â”€â”€ +input_layernorm,post_attention_layernorm(RMSNorm) weight[4096](fp16)(cuda:0)â„ï¸
â”‚   â””â”€â”€ norm(RMSNorm) weight[4096](fp16)(cuda:0)â„ï¸
â””â”€â”€ lm_head(CastOutputToFloat)
    â””â”€â”€ 0(Linear) weight[32000,4096](fp16)(cuda:0)â„ï¸
Number of parameters: 6742364160
Processing C4 Samples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:06<00:00, 19.31it/s]
Benchmarking: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:50<00:00, 20.45it/s]
Median: 0.04806506633758545
PPL: 2063.745849609375
Max memory(MiB): 2587.87646484375
```
